---
title: "Collective Autonomous Air Mobility Systems"
excerpt: "Encouraging emergent coordination in collective autonomous air mobility systems."
header:
    teaser: /assets/images/research/uam.jpeg
tags:
  - artificial intelligence
  - intelligent transportation systems
  - multi-agent systems
  - reinforcement learning
  - robotics
last_modified_at: 2025-01-17
---

We work on methods for coordinating many agents in collective autonomous air mobility systems (CAAMS), like urban air mobility and drone firefighting. We specifically aim to encourage emergent cooperation in CAAMS, with consideration of heterogeneity (e.g., individual objectives, capabilities) and the overall integrated system (e.g., vehicles and their interactions with operators and services).
<!-- Our key ideas are to learn from existing collective autonomous sytems (eCAS), like autonomous vehicles and biological systems, and leverage insights gained from such systems to create new multi-agent reinforcement learning (MARL) algorithms. -->

This work is funded in part by NASA 80NSSC23M0221 and ONR N00014-20-1-2249.

<div class="funding-logos">
    <a href="https://www.nasa.gov/"><img src="{{ site.url }}{{ site.baseurl }}/assets/images/funding/nasa.jpg"></a>
    <a href="https://www.onr.navy.mil/"><img src="{{ site.url }}{{ site.baseurl }}/assets/images/funding/onr.png"></a>
</div>

## Predicting social cost of self-interested agents

<figure-full-caption-very-large>
	<a href="{{ site.url }}{{ site.baseurl }}/assets/images/research/spoa.png"><img src="{{ site.url }}{{ site.baseurl }}/assets/images/research/spoa.png"></a>
	<figcaption>High-level view of our proposed approach for understanding the inefficiency of Markov game agents. Our key idea is to estimate the inefficiency of self-interested agents with respect to a social objective using a state-dependent formulation of price of anarchy.</figcaption>
</figure-full-caption-very-large>

This work explores ways to evaluate and predict the social cost induced by agents acting according to their own selfish objectives. Our approach leverages value functions from policies optimized for the social objective, modeled as a multi-agent Markov decision process (MMDP), and individual agent objectives, modeled as a Markov game (MG), to **predict a social cost metric for any given state.** See our 2025 AIAA SciTech Forum paper for more details.

<div class="row">
    <a href="https://arc.aiaa.org/doi/10.2514/6.2025-1929" class="button_general">DOI</a>
</div>



## A survey of applications of inverse reinforcement learning in aviation

<figure-full-caption-very-large>
	<a href="{{ site.url }}{{ site.baseurl }}/assets/images/research/aviation-irl.png"><img src="{{ site.url }}{{ site.baseurl }}/assets/images/research/aviation-irl.png"></a>
	<figcaption>Our work was motivated by the finding that the use of IRL for aviation has not followed trends seen in other domains like autonomous vehicles. This figure shows the number of IRL papers published across engineering and specifically in aerospace engineering. Data was retrieved from Dimensions, a database of research articles indexed via Crossref, PubMed, PubMed Central, arXiv.org and more than 160 publishers directly. Papers counted here include those containing the key phrase “inverse reinforcement learning.” Aerospace engineering papers were filtered using Dimension’s research area subcategory “Aerospace Engineering.</figcaption>
</figure-full-caption-very-large>

This work surveys current applications of inverse reinforcement learning (IRL) in aviation. We also identify potential challenges of using IRL for aviation, which may explain its current limited use within the field, and identify potential future applications of IRL for aviation. See our 2025 AIAA SciTech Forum paper for more details.

<div class="row">
    <a href="https://arc.aiaa.org/doi/10.2514/6.2025-1540" class="button_general">DOI</a>
</div>
