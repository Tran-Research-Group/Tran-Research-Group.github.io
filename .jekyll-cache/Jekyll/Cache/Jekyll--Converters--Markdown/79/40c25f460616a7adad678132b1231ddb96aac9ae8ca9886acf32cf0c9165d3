I"¾<p>We work on multi-agent reinforcement learning (MARL) methods to create teams of autonomous agents that can work together to solve challenging tasks, such as search-and-rescue, traffic control, autonomous driving, and warehouse robotics.</p>

<h2 id="improving-decentralized-training-with-successor-features">Improving decentralized training with successor features</h2>

<video muted="" autoplay="autoplay" loop="loop" width="500px" controls="">
  <source src="/assets/videos/dissc.mp4" type="video/mp4" />
</video>

<p>This work leverages successor features (SFs) to <strong>disentangle</strong> an individual agentâ€™s impact on the global value function from that of all other agents, thus enabling better training of decentralized agents. The video shows an example of gameplay from the Starcraft Multi-Agent Challenge (SMAC) where the red agents learned to surround the blue agents and win the game, despite being at a numbers disadvantage (27 vs. 30 agents). See our AAMAS paper presenting our approach, named <strong>DISentangling Successor features for Coordination (DISSC)</strong>, for more details.</p>

<div class="row">
    <a href="https://doi.org/10.48550/arXiv.2202.07741" class="button_general">arXiv</a>
    <!-- <a href="#" class="button_general">GitHub</a> -->
</div>

<!-- ## Learning multi-agent roles

This work builds from [DISSC](#improving-decentralized-training-with-successor-features) by leveraging SFs to learn **roles** that individual agents can use within a team, inspired by the use of different positions in team sports like football and basketball. Our preliminary results show **TODO**. -->

<!-- ## Leveraging communication through coordination graphs

This work aims to **TODO**. -->
:ET